# Machine Learning
<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning.jpg" alt="Logo" width="720" height="360">
</a></p>

<!-- TABLE OF CONTENTS -->

## Table of Contents
* [1. Introduction of Deep Learning](#introduction_of_Deep_Learning)
* [2. Tips for Training DNN](#tips_for_Training_DNN)  
 &nbsp;&nbsp;&nbsp;&nbsp;  [2. Virtual Environment](#virtual_environment)  



<!-- Introduction of Deep Learning -->
## Introduction_of_Deep_Learning

### Some definations
* **Neuron**  
<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/1.png" alt="Logo" width="720" height="480">
</a></p>

* **Activation Function**  
<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/2.png" alt="Logo" width="720" height="480">
</a></p>  

* **Neural Network**  

* **Weights**  

* **Bias**   

* **Activation function** 

* **Network parameters**   
Weights and biases are network parameters Œ∏.   

* **Feedforward neural network** 
A feedforward neural network is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from its descendant: recurrent neural networks.
The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/3.png" alt="Logo" width="720" height="480">
</a></p>

* **Softmax Layer**
The [softmax function](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)  is a function that turns a vector of K real values into a vector of K real values that sum to 1.

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/4.png" alt="Logo" width="720" height="480">
</a></p>

* **Loss Function**

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/5.png" alt="Logo" width="720" height="480">
</a></p>

* **Gradient Descent**
[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. 

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/6.png" alt="Logo" width="720" height="480">
</a></p>

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/7.png" alt="Logo" width="720" height="480">
</a></p>

<p align="center"><a href="https://github.com/Grindewald1900/Notebook">
    <img src="/Image/Machine Learning/8.png" alt="Logo" width="720" height="480">
</a></p>

* **Backpropagation**
In machine learning, [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) is a widely used algorithm in training feedforward neural networks for supervised learning.  
It is also an efficient way to compute ùúïùêø /ùúïùë§.

### Three Steps for Deep Learning   
* **1. Define a set of function**  
* **2. Estimate goodness of function**  
* **3. Pick the best function**  


## Tips_for_Training_DNN

### Keras
[Keras](https://keras.io/getting_started/) is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.  

